{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f45bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import os, glob, random\n",
    "from io import open\n",
    "import unicodedata, string\n",
    "# thanks to https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858d5007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset/data/names/Czech.txt', '../dataset/data/names/German.txt', '../dataset/data/names/Arabic.txt', '../dataset/data/names/Japanese.txt', '../dataset/data/names/Chinese.txt', '../dataset/data/names/Vietnamese.txt', '../dataset/data/names/Russian.txt', '../dataset/data/names/French.txt', '../dataset/data/names/Irish.txt', '../dataset/data/names/English.txt', '../dataset/data/names/Spanish.txt', '../dataset/data/names/Greek.txt', '../dataset/data/names/Italian.txt', '../dataset/data/names/Portuguese.txt', '../dataset/data/names/Scottish.txt', '../dataset/data/names/Dutch.txt', '../dataset/data/names/Korean.txt', '../dataset/data/names/Polish.txt']\n",
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "def find_files(path): \n",
    "    return glob.glob(path)\n",
    "print(find_files('../dataset/data/names/*.txt'))\n",
    "\n",
    "def unicode_to_ascii(s, all_letters):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in all_letters\n",
    "    )\n",
    "\n",
    "all_letters = string.ascii_letters + \".,;''\"\n",
    "n_letters = len(all_letters)\n",
    "print(unicode_to_ascii('Ślusàrski', all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebeeeb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n",
      "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def read_lines(filename):\n",
    "    f = open(filename, encoding='utf-8')\n",
    "    content = f.read().strip().split('\\n')  # good trick to make document into list, strip() remove leading and trailing space.  \n",
    "    res = [unicode_to_ascii(name, all_letters) for name in content]\n",
    "    return res\n",
    "\n",
    "def categorize(path = '../dataset/data/names/*.txt'):\n",
    "    category_lines = collections.defaultdict(list)\n",
    "  \n",
    "    pathes = find_files(path)\n",
    "    for filename in pathes:\n",
    "        basename = os.path.basename(filename)      # output Chinese.txt, this is removing leading directory. \n",
    "        category = os.path.splitext(basename)[0]   # this splits the filename and the extension. \n",
    "        all_categories.append(category)\n",
    "        lines = read_lines(filename)\n",
    "        category_lines[category] = lines\n",
    "    return category_lines\n",
    "all_categories = []\n",
    "category_lines = categorize()\n",
    "# print(category_lines)\n",
    "n_categories = len(all_categories)\n",
    "print(n_categories)                               \n",
    "print(category_lines[\"Italian\"][:5]) \n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b765a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def letter2idx(letter): return all_letters.find(letter)\n",
    "def letter2tensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letter2idx(letter)] = 1\n",
    "    return tensor\n",
    "def line2tensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)  # tensor treate everything with batch\n",
    "    for i, char in enumerate(line):\n",
    "        tensor[i][0][letter2idx(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letter2tensor('J'))\n",
    "print(line2tensor(\"adfadf\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9516981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8094, -2.9531, -2.8433, -2.9077, -2.8186, -2.9677, -2.7931, -2.9278,\n",
      "         -2.8185, -2.8538, -2.9698, -2.8808, -2.9851, -2.7302, -2.9611, -2.9959,\n",
      "         -2.9263, -2.9360]], grad_fn=<LogSoftmaxBackward>) tensor([[ 0.0700, -0.0099, -0.0380, -0.0079,  0.0454, -0.0385, -0.0488, -0.0493,\n",
      "         -0.0119, -0.0605, -0.0175, -0.0221, -0.0409, -0.0326, -0.0334,  0.0094,\n",
      "          0.0897, -0.0034, -0.0822, -0.0094,  0.0217,  0.0517,  0.0023,  0.0040,\n",
      "          0.0338, -0.1036,  0.0050, -0.0205, -0.0647,  0.0565, -0.0561,  0.0600,\n",
      "          0.0065, -0.0128, -0.0742, -0.0069, -0.0237,  0.0272,  0.0834,  0.1100,\n",
      "         -0.0913,  0.0543, -0.0183, -0.0182, -0.1311, -0.0282, -0.1155, -0.0439,\n",
      "          0.1229,  0.0881, -0.0957, -0.0043, -0.1153,  0.0068,  0.0647,  0.0514,\n",
      "         -0.0083,  0.0761,  0.1220,  0.1306,  0.0141, -0.0938, -0.0024, -0.1040,\n",
      "         -0.0674, -0.0522, -0.0859, -0.0322,  0.0233, -0.0327, -0.0321, -0.0084,\n",
      "          0.0194, -0.0675,  0.0229, -0.0165,  0.0688, -0.0852,  0.0948,  0.0865,\n",
      "          0.1401, -0.0583,  0.0492, -0.0509,  0.0482,  0.0077, -0.0671, -0.0207,\n",
      "          0.0964,  0.0184, -0.0008,  0.0303, -0.0302, -0.1280,  0.0311, -0.0314,\n",
      "          0.0280,  0.0911, -0.0547,  0.0019,  0.0823, -0.0143,  0.0093, -0.0816,\n",
      "          0.0085, -0.0370, -0.0020,  0.0250, -0.0167,  0.0452,  0.0572, -0.0044,\n",
      "         -0.0676, -0.0548,  0.0571,  0.0599, -0.0483,  0.0860,  0.1304, -0.0101,\n",
      "          0.1025,  0.1041, -0.0317,  0.0584, -0.0289,  0.1361,  0.0251,  0.0007]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "activations = nn.ModuleDict([\n",
    "    ['lrelu', nn.LeakyReLU()],\n",
    "    ['relu', nn.ReLU()],\n",
    "    ['selu', nn.SELU(inplace=True)],\n",
    "    ['tanh', nn.Tanh()],\n",
    "    ['softmax', nn.LogSoftmax(dim=1)],\n",
    "    ['none', nn.Identity()],\n",
    "])\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "#         self.activation = nn.LogSoftmax(dim=1)\n",
    "        self.activation = activations['softmax']\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined) \n",
    "        output = self.i2o(combined)\n",
    "        output = self.activation(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "# input = letter2tensor('A')\n",
    "# hidden = torch.zeros(1, n_hidden)\n",
    "# output, next_hidden = rnn(input, hidden)\n",
    "# print(output, next_hidden)\n",
    "\n",
    "input = line2tensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output, next_hidden)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151a16d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Portuguese', 13)\n",
      "category = Japanese / line =  Kijimuta\n",
      "category = Chinese / line =  Jing\n",
      "category = Japanese / line =  Yamada\n",
      "category = German / line =  Bumgarner\n",
      "category = French / line =  Sauvage\n",
      "category = Spanish / line =  Araullo\n",
      "category = Greek / line =  Antimisiaris\n",
      "category = French / line =  Segal\n",
      "category = Scottish / line =  Mclean\n",
      "category = Korean / line =  So\n"
     ]
    }
   ],
   "source": [
    "def category_from_output(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "print(category_from_output(output))\n",
    "\n",
    "def random_choice(arr):\n",
    "    return arr[random.randint(0, len(arr) - 1)]\n",
    "\n",
    "def get_training_sample():\n",
    "    category = random_choice(all_categories)\n",
    "    line = random_choice(category_lines[category])\n",
    "    line_tensor = line2tensor(line)\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = get_training_sample()\n",
    "    print('category =', category, '/ line = ', line )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c888c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0% (0m 1s) 40.3232 Wray / Polish ✗ (English)\n",
      "1000 1% (0m 3s) 272931.8750 Sternberg / Japanese ✗ (German)\n",
      "1500 1% (0m 5s) 398.3838 Honda / Dutch ✗ (Japanese)\n",
      "2000 2% (0m 6s) 11612.6338 Legrand / Arabic ✗ (French)\n",
      "2500 2% (0m 8s) 262.2949 Gosse / Russian ✗ (French)\n",
      "3000 3% (0m 10s) 6402746.0000 Awinowitski / Dutch ✗ (Russian)\n",
      "3500 3% (0m 11s) 11985252.0000 Lichtenberg / Dutch ✗ (German)\n",
      "4000 4% (0m 13s) 10.3745 Han / French ✗ (Chinese)\n",
      "4500 4% (0m 15s) 525.7008 Doble / German ✗ (English)\n",
      "5000 5% (0m 17s) 8.2079 Phi / Chinese ✗ (Vietnamese)\n",
      "5500 5% (0m 18s) 595939.4375 Ishinomori / Portuguese ✗ (Japanese)\n",
      "6000 6% (0m 20s) 0.0000 Wen / Chinese ✓\n",
      "6500 6% (0m 22s) 0.0000 Romao / Portuguese ✓\n",
      "7000 7% (0m 23s) 10049.0117 Agthoven / Chinese ✗ (Dutch)\n",
      "7500 7% (0m 25s) 319.6494 Simon / Polish ✗ (French)\n",
      "8000 8% (0m 27s) 3254420.0000 Goldschmidt / Greek ✗ (German)\n",
      "8500 8% (0m 28s) 1668.0435 Minami / Dutch ✗ (Japanese)\n",
      "9000 9% (0m 31s) 18771.1133 Hamilton / English ✗ (Scottish)\n",
      "9500 9% (0m 33s) 160.3038 Villa / French ✗ (Spanish)\n",
      "10000 10% (0m 35s) 596.8656 Soares / English ✗ (Portuguese)\n",
      "10500 10% (0m 37s) 36021.1328 Wedekind / Irish ✗ (German)\n",
      "11000 11% (0m 38s) 272.5641 Nunes / Greek ✗ (Portuguese)\n",
      "11500 11% (0m 40s) 4271.1021 Saville / German ✗ (English)\n",
      "12000 12% (0m 43s) 2225539.0000 Mokhnatsky / Portuguese ✗ (Russian)\n",
      "12500 12% (0m 45s) 17.1799 Dam / Polish ✗ (Vietnamese)\n",
      "13000 13% (0m 47s) 1.2193 Eng / Korean ✗ (Chinese)\n",
      "13500 13% (0m 48s) 610.2410 Amari / Greek ✗ (Arabic)\n",
      "14000 14% (0m 50s) 19.6086 Ikeda / Chinese ✗ (Japanese)\n",
      "14500 14% (0m 52s) 1197.3918 Naifeh / Dutch ✗ (Arabic)\n",
      "15000 15% (0m 54s) 1149.4586 Porras / Chinese ✗ (Spanish)\n",
      "15500 15% (0m 55s) 16.3739 Luu / Korean ✗ (Vietnamese)\n",
      "16000 16% (0m 57s) 246106.9375 Henriques / Irish ✗ (Portuguese)\n",
      "16500 16% (0m 59s) 3058.0725 Gabler / Czech ✗ (German)\n",
      "17000 17% (1m 0s) 335.6966 White / Korean ✗ (Scottish)\n",
      "17500 17% (1m 2s) 42669.5312 Quintana / German ✗ (Spanish)\n",
      "18000 18% (1m 5s) 1051.3218 Young / Japanese ✗ (Scottish)\n",
      "18500 18% (1m 6s) 34459.7109 Handzlik / Italian ✗ (Czech)\n",
      "19000 19% (1m 8s) 33437.4531 Stramba / German ✗ (Czech)\n",
      "19500 19% (1m 10s) 195673.6562 Kajiwara / German ✗ (Japanese)\n",
      "20000 20% (1m 11s) 296.2152 Daal / Irish ✗ (Dutch)\n",
      "20500 20% (1m 13s) 22.6843 Dai / Polish ✗ (Chinese)\n",
      "21000 21% (1m 17s) 20.4007 Vuu / Korean ✗ (Vietnamese)\n",
      "21500 21% (1m 20s) 1148.7139 Nunes / Irish ✗ (Portuguese)\n",
      "22000 22% (1m 21s) 12091.7441 Murphy / Arabic ✗ (Scottish)\n",
      "22500 22% (1m 23s) 22754.7148 Fritsch / Irish ✗ (Czech)\n",
      "23000 23% (1m 26s) 13976.3633 Rzhavin / Arabic ✗ (Russian)\n",
      "23500 23% (1m 29s) 11.1639 Ha / Japanese ✗ (Vietnamese)\n",
      "24000 24% (1m 31s) 1065.1300 Emile / Arabic ✗ (French)\n",
      "24500 24% (1m 33s) 18382.0859 Romeijn / Irish ✗ (Dutch)\n",
      "25000 25% (1m 35s) 89.2970 Vann / Russian ✗ (Dutch)\n",
      "25500 25% (1m 37s) 9.8655 Qin / Vietnamese ✗ (Chinese)\n",
      "26000 26% (1m 39s) 2604.7424 Kassab / Dutch ✗ (Arabic)\n",
      "26500 26% (1m 40s) 16279.2480 Dufour / Greek ✗ (French)\n",
      "27000 27% (1m 42s) 659.2101 Vlach / Russian ✗ (Czech)\n",
      "27500 27% (1m 44s) 1527.4247 Thien / Portuguese ✗ (Chinese)\n",
      "28000 28% (1m 46s) 183347.2188 Maclean / Portuguese ✗ (Scottish)\n",
      "28500 28% (1m 48s) 16720.2480 Crespo / French ✗ (Spanish)\n",
      "29000 28% (1m 49s) 0.0000 Segers / Dutch ✓\n",
      "29500 29% (1m 51s) 107762.6484 Gaffney / Dutch ✗ (English)\n",
      "30000 30% (1m 53s) 1813.9329 Turati / French ✗ (Italian)\n",
      "30500 30% (1m 55s) 122582.7188 Tatnell / Dutch ✗ (English)\n",
      "31000 31% (1m 56s) 150919040.0000 Angelopoulos / Vietnamese ✗ (Greek)\n",
      "31500 31% (1m 58s) 31346184.0000 Auttenberg / German ✗ (Polish)\n",
      "32000 32% (2m 1s) 1499627.8750 Fiscella / Russian ✗ (Italian)\n",
      "32500 32% (2m 4s) 0.0000 Seaghdha / Irish ✓\n",
      "33000 33% (2m 7s) 34.5043 Watt / German ✗ (English)\n",
      "33500 33% (2m 12s) 11563.5137 Aquino / Polish ✗ (Spanish)\n",
      "34000 34% (2m 17s) 2356.7729 Nahas / Dutch ✗ (Arabic)\n",
      "34500 34% (2m 20s) 18850.7637 Turati / Irish ✗ (Italian)\n",
      "35000 35% (2m 22s) 1032.4559 Okuma / Czech ✗ (Japanese)\n",
      "35500 35% (2m 23s) 486.4032 Gaur / German ✗ (Russian)\n",
      "36000 36% (2m 25s) 2265.3633 Chung / German ✗ (Vietnamese)\n",
      "36500 36% (2m 27s) 47999.2461 Chellos / Russian ✗ (Greek)\n",
      "37000 37% (2m 29s) 7975593.0000 Nurgaleev / Irish ✗ (Russian)\n",
      "37500 37% (2m 31s) 3009401.2500 Ninomiya / Czech ✗ (Japanese)\n",
      "38000 38% (2m 32s) 36537712640.0000 Konstantatos / Spanish ✗ (Greek)\n",
      "38500 38% (2m 34s) 101443.1562 Ventura / German ✗ (Portuguese)\n",
      "39000 39% (2m 36s) 487521.1250 Golosenin / Arabic ✗ (Russian)\n",
      "39500 39% (2m 37s) 1966.4741 Jeong / Russian ✗ (Korean)\n",
      "40000 40% (2m 39s) 15561.2422 Yankov / Greek ✗ (Russian)\n",
      "40500 40% (2m 41s) 2743852544.0000 Schwinghammer / Dutch ✗ (German)\n",
      "41000 41% (2m 43s) 1312376.3750 Bakhorin / German ✗ (Russian)\n",
      "41500 41% (2m 45s) 14503.0664 Whelan / German ✗ (Irish)\n",
      "42000 42% (2m 46s) 617.8313 Sabol / Arabic ✗ (Czech)\n",
      "42500 42% (2m 48s) 152.0201 Gauk / German ✗ (Chinese)\n",
      "43000 43% (2m 50s) 3587.0024 Neuman / Scottish ✗ (German)\n",
      "43500 43% (2m 51s) 5232346.0000 Aliprandi / Russian ✗ (Italian)\n",
      "44000 44% (2m 53s) 0.0000 Fourakis / Greek ✓\n",
      "44500 44% (2m 55s) 173.7179 Yoon / Vietnamese ✗ (Korean)\n",
      "45000 45% (2m 57s) 294297.3750 Yasuhiro / Greek ✗ (Japanese)\n",
      "45500 45% (2m 59s) 0.0000 TerAvest / Dutch ✓\n",
      "46000 46% (3m 0s) 314.3220 Asker / Dutch ✗ (Arabic)\n",
      "46500 46% (3m 2s) 6.2017 Ku / Vietnamese ✗ (Korean)\n",
      "47000 47% (3m 4s) 7457.5854 Dudnik / Greek ✗ (Russian)\n",
      "47500 47% (3m 5s) 21.1583 Cao / Spanish ✗ (Vietnamese)\n",
      "48000 48% (3m 7s) 42.7620 Byon / Vietnamese ✗ (Korean)\n",
      "48500 48% (3m 9s) 16050744.0000 Meeuwessen / English ✗ (Dutch)\n",
      "49000 49% (3m 10s) 0.0000 Rousses / Greek ✓\n",
      "49500 49% (3m 12s) 0.0000 O'Connell / Irish ✓\n",
      "50000 50% (3m 14s) 24671820.0000 Rooijakker / Greek ✗ (Dutch)\n",
      "50500 50% (3m 16s) 78511408.0000 Shigemitsu / Greek ✗ (Japanese)\n",
      "51000 51% (3m 17s) 5105.4111 Klimek / Dutch ✗ (Polish)\n",
      "51500 51% (3m 19s) 2608.8521 Daher / Polish ✗ (Arabic)\n",
      "52000 52% (3m 21s) 323.8018 Dinh / Japanese ✗ (Vietnamese)\n",
      "52500 52% (3m 22s) 83844.3906 Azarola / Irish ✗ (Spanish)\n",
      "53000 53% (3m 24s) 9259.1455 Nolan / Greek ✗ (Irish)\n",
      "53500 53% (3m 26s) 859.3477 Pinho / Greek ✗ (Portuguese)\n",
      "54000 54% (3m 27s) 7.3612 Byon / Greek ✗ (Korean)\n",
      "54500 54% (3m 29s) 229348.4062 Ventura / Irish ✗ (Portuguese)\n",
      "55000 55% (3m 31s) 13.3345 Ton / Korean ✗ (Vietnamese)\n",
      "55500 55% (3m 32s) 0.0000 Nani / Italian ✓\n",
      "56000 56% (3m 34s) 2834.3411 Negri / Irish ✗ (Italian)\n",
      "56500 56% (3m 36s) 8878004.0000 Kobayashi / Irish ✗ (Japanese)\n",
      "57000 56% (3m 38s) 1621.6550 Phung / Irish ✗ (Vietnamese)\n",
      "57500 57% (3m 39s) 45995.5781 Miller / Russian ✗ (Scottish)\n",
      "58000 57% (3m 41s) 4357.2734 Totolos / Czech ✗ (Greek)\n",
      "58500 58% (3m 43s) 36684.8633 Richard / Irish ✗ (French)\n",
      "59000 59% (3m 45s) 0.0000 Lyzlov / Russian ✓\n",
      "59500 59% (3m 46s) 0.0000 Dalach / Irish ✓\n",
      "60000 60% (3m 48s) 9040.2852 Samaha / Russian ✗ (Arabic)\n",
      "60500 60% (3m 50s) 57065.5469 Barros / Polish ✗ (Portuguese)\n",
      "61000 61% (3m 52s) 26037.4707 Garcia / Irish ✗ (Portuguese)\n",
      "61500 61% (3m 53s) 382.7626 Banh / Dutch ✗ (Vietnamese)\n",
      "62000 62% (3m 55s) 7499.3218 Nguyen / Greek ✗ (Vietnamese)\n",
      "62500 62% (3m 57s) 575.5515 Lind / Irish ✗ (Czech)\n",
      "63000 63% (3m 59s) 2048.3633 Reagan / French ✗ (Irish)\n",
      "63500 63% (4m 1s) 20511.1816 Pavone / Irish ✗ (Italian)\n",
      "64000 64% (4m 2s) 14.0322 Ko / Portuguese ✗ (Korean)\n",
      "64500 64% (4m 4s) 1986.2444 Porra / Japanese ✗ (Spanish)\n",
      "65000 65% (4m 6s) 6171.8901 Names / Greek ✗ (Irish)\n",
      "65500 65% (4m 8s) 2992.3779 Jordan / Dutch ✗ (Polish)\n",
      "66000 66% (4m 9s) 4255519.0000 Thorndyke / Korean ✗ (English)\n",
      "66500 66% (4m 11s) 1554327.2500 Fontaine / Greek ✗ (French)\n",
      "67000 67% (4m 13s) 176.6565 Tong / Russian ✗ (Vietnamese)\n",
      "67500 67% (4m 14s) 7115.3896 Wexler / Russian ✗ (German)\n",
      "68000 68% (4m 16s) 13397.7402 Kaufer / Dutch ✗ (German)\n",
      "68500 68% (4m 18s) 829.8480 Danas / Czech ✗ (Greek)\n",
      "69000 69% (4m 19s) 346.3975 Ross / Portuguese ✗ (Scottish)\n",
      "69500 69% (4m 21s) 2515.0386 Teagan / Italian ✗ (Irish)\n",
      "70000 70% (4m 23s) 1129568.0000 Peltsman / Greek ✗ (Russian)\n",
      "70500 70% (4m 24s) 197182.9844 Richard / Dutch ✗ (German)\n",
      "71000 71% (4m 26s) 866050.6250 Sierzant / German ✗ (Polish)\n",
      "71500 71% (4m 28s) 0.0000 Greenway / English ✓\n",
      "72000 72% (4m 29s) 2919.0662 Handal / Spanish ✗ (Arabic)\n",
      "72500 72% (4m 31s) 0.0000 Koeman / Dutch ✓\n",
      "73000 73% (4m 32s) 178.7083 Reid / Czech ✗ (Scottish)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73500 73% (4m 34s) 238.4543 Kaluza / Dutch ✗ (Polish)\n",
      "74000 74% (4m 36s) 930.2970 Abreu / Spanish ✗ (Portuguese)\n",
      "74500 74% (4m 38s) 0.0000 Varela / Spanish ✓\n",
      "75000 75% (4m 39s) 16.7465 Baz / Vietnamese ✗ (Arabic)\n",
      "75500 75% (4m 41s) 12850.8525 Hakimi / Spanish ✗ (Arabic)\n",
      "76000 76% (4m 43s) 51549.9727 Tritten / Greek ✗ (German)\n",
      "76500 76% (4m 45s) 8610.1426 Kruger / Dutch ✗ (German)\n",
      "77000 77% (4m 46s) 0.0000 Aart / Dutch ✓\n",
      "77500 77% (4m 49s) 84.9270 Chun / English ✗ (Korean)\n",
      "78000 78% (4m 50s) 149946.9844 Jeronkin / Italian ✗ (Russian)\n",
      "78500 78% (4m 52s) 67163912.0000 Bartolomei / English ✗ (Italian)\n",
      "79000 79% (4m 54s) 83245.5469 Michel / English ✗ (Spanish)\n",
      "79500 79% (4m 56s) 45953636.0000 Breitbarth / Polish ✗ (German)\n",
      "80000 80% (4m 57s) 185.6261 Gong / Arabic ✗ (Chinese)\n",
      "80500 80% (4m 59s) 83187.9922 Salazar / Dutch ✗ (Portuguese)\n",
      "81000 81% (5m 1s) 13333.8623 Piontek / Dutch ✗ (Polish)\n",
      "81500 81% (5m 2s) 389.0654 Negri / English ✗ (Italian)\n",
      "82000 82% (5m 4s) 11604.2930 Salomon / Dutch ✗ (Polish)\n",
      "82500 82% (5m 5s) 28109.2734 Kouros / Arabic ✗ (Greek)\n",
      "83000 83% (5m 7s) 2.8977 Jue / Scottish ✗ (Chinese)\n",
      "83500 83% (5m 9s) 569171.5000 Kijimuta / Dutch ✗ (Japanese)\n",
      "84000 84% (5m 10s) 179933.7656 Vicario / Greek ✗ (Italian)\n",
      "84500 84% (5m 12s) 298673792.0000 Slusarczyk / Irish ✗ (Polish)\n",
      "85000 85% (5m 14s) 35.1161 Bao / Vietnamese ✗ (Chinese)\n",
      "85500 85% (5m 15s) 197068.5938 Kalaida / Portuguese ✗ (Russian)\n",
      "86000 86% (5m 17s) 7553.1841 Goebel / Russian ✗ (German)\n",
      "86500 86% (5m 19s) 319.1945 Grec / Vietnamese ✗ (Spanish)\n",
      "87000 87% (5m 21s) 149.3131 Baik / Dutch ✗ (Korean)\n",
      "87500 87% (5m 23s) 11.1349 Jong / Dutch ✗ (Korean)\n",
      "88000 88% (5m 25s) 1979.1797 Macleod / Dutch ✗ (Scottish)\n",
      "88500 88% (5m 26s) 4763172.0000 Pokhodeev / German ✗ (Russian)\n",
      "89000 89% (5m 28s) 171306.8438 Mullins / Dutch ✗ (French)\n",
      "89500 89% (5m 30s) 121.2612 Thao / Portuguese ✗ (Vietnamese)\n",
      "90000 90% (5m 32s) 1071.6282 Ebner / Vietnamese ✗ (German)\n",
      "90500 90% (5m 34s) 363591999946752.0000 Panayiotopoulos / Russian ✗ (Greek)\n",
      "91000 91% (5m 36s) 518430.5938 Donnell / Arabic ✗ (Irish)\n",
      "91500 91% (5m 38s) 0.0000 Sanna / Dutch ✓\n",
      "92000 92% (5m 40s) 220441472.0000 Ferreiro / Greek ✗ (Portuguese)\n",
      "92500 92% (5m 41s) 45594.4375 Michel / Arabic ✗ (Polish)\n",
      "93000 93% (5m 43s) 4374725.5000 Aliberti / German ✗ (Italian)\n",
      "93500 93% (5m 45s) 8711.2480 Casey / Dutch ✗ (Irish)\n",
      "94000 94% (5m 48s) 133021.6562 Ogterop / Scottish ✗ (Dutch)\n",
      "94500 94% (5m 50s) 423664.8750 Bekyros / Polish ✗ (Greek)\n",
      "95000 95% (5m 53s) 2825001.5000 Sokoloff / French ✗ (Polish)\n",
      "95500 95% (5m 54s) 3609679.0000 Hanraets / German ✗ (Dutch)\n",
      "96000 96% (5m 56s) 2487.2441 OwYang / German ✗ (Chinese)\n",
      "96500 96% (5m 59s) 4320515.0000 Drivakis / Scottish ✗ (Greek)\n",
      "97000 97% (6m 1s) 3663280.2500 Takasugi / German ✗ (Japanese)\n",
      "97500 97% (6m 3s) 0.0000 Ruadhan / Irish ✓\n",
      "98000 98% (6m 5s) 144325520.0000 Papadopulos / Czech ✗ (Greek)\n",
      "98500 98% (6m 6s) 117035.2500 Forakis / Irish ✗ (Greek)\n",
      "99000 99% (6m 8s) 30170.0859 Szwarc / Scottish ✗ (Polish)\n",
      "99500 99% (6m 10s) 183.2735 Kwak / Dutch ✗ (Korean)\n",
      "100000 100% (6m 12s) 4382.9409 Stamp / Dutch ✗ (English)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(line_tensor, category_tensor):\n",
    "    hidden = rnn.initHidden()   # note: hidden layer needs to initilize each time trian with a word, get each letter and pass sequencly to with hidden layer. \n",
    "    rnn.train()\n",
    "#     rnn.zero_grad()  # note, need to zero_grad outside the for loop, one zero when finish one word\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(line_tensor.size()[0]):  # number of letters in this word\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "#     for p in rnn.parameters():   # these two line can replace optimizer\n",
    "#         p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    \n",
    "    optimizer.step()\n",
    "    return output, loss.item()\n",
    "\n",
    "import time, math\n",
    "n_iters = 100000\n",
    "print_every = 500\n",
    "plot_every = 100\n",
    "\n",
    "def time_elapse(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' %(m, s)\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    \n",
    "    category, line, category_tensor, line_tensor = get_training_sample()\n",
    "    output, loss = train(line_tensor, category_tensor)\n",
    "    current_loss += loss\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        pred, pred_idx = category_from_output(output)\n",
    "        correct = '✓' if pred == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, \n",
    "                    time_elapse(start), loss, line, pred, correct))\n",
    "        \n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "# print(all_losses)\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eadefc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-05de63a09154>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-05de63a09154>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    with torch.no_grad()\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "def evalute(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(line_tensor.size()[0]):\n",
    "            output, hidden = rnn(line_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = get_training_sample()\n",
    "    output = evalute(line_tensor)\n",
    "#     guess, guess_i = category_from_output(output)\n",
    "#     category_i = all_categories.index(category)\n",
    "#     confusion[category_i][guess_i] += 1\n",
    "    \n",
    "# for i in range(n_categoreis):\n",
    "#     confusion[i] = confusion[i] / confusion[i].sum()\n",
    "    \n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(confusion.numpy())\n",
    "# fig.colorbar(cax)\n",
    "# ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "# ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# # Force label at every tick\n",
    "# ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "# ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# # sphinx_gallery_thumbnail_number = 2\n",
    "# plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f1e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc19675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353bc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530b983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6b4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab5868f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6756a315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea756a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77e412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69efbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11230128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
